{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the OpenAI library. This library provides a Python interface to the OpenAI API.\n",
    "from openai import OpenAI\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "from vosk import Model, KaldiRecognizer\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Say something!\n",
      "You said: hello how is your day hello\n",
      "Say something!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sr\u001b[38;5;241m.\u001b[39mMicrophone() \u001b[38;5;28;01mas\u001b[39;00m source:\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSay something!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m     audio \u001b[38;5;241m=\u001b[39m \u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlisten\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m         \u001b[38;5;66;03m# Use Google's Speech Recognition API to convert the audio to text\u001b[39;00m\n\u001b[0;32m     15\u001b[0m         text \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mrecognize_google(audio)\n",
      "File \u001b[1;32mc:\\LTAM PC\\teinnStuff\\SpeechToImage_Project\\.venv\\Lib\\site-packages\\speech_recognition\\__init__.py:523\u001b[0m, in \u001b[0;36mRecognizer.listen\u001b[1;34m(self, source, timeout, phrase_time_limit, snowboy_configuration)\u001b[0m\n\u001b[0;32m    520\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m phrase_time_limit \u001b[38;5;129;01mand\u001b[39;00m elapsed_time \u001b[38;5;241m-\u001b[39m phrase_start_time \u001b[38;5;241m>\u001b[39m phrase_time_limit:\n\u001b[0;32m    521\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 523\u001b[0m buffer \u001b[38;5;241m=\u001b[39m \u001b[43msource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCHUNK\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# reached end of the stream\u001b[39;00m\n\u001b[0;32m    525\u001b[0m frames\u001b[38;5;241m.\u001b[39mappend(buffer)\n",
      "File \u001b[1;32mc:\\LTAM PC\\teinnStuff\\SpeechToImage_Project\\.venv\\Lib\\site-packages\\speech_recognition\\__init__.py:199\u001b[0m, in \u001b[0;36mMicrophone.MicrophoneStream.read\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread\u001b[39m(\u001b[38;5;28mself\u001b[39m, size):\n\u001b[1;32m--> 199\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpyaudio_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexception_on_overflow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\LTAM PC\\teinnStuff\\SpeechToImage_Project\\.venv\\Lib\\site-packages\\pyaudio\\__init__.py:570\u001b[0m, in \u001b[0;36mPyAudio.Stream.read\u001b[1;34m(self, num_frames, exception_on_overflow)\u001b[0m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_input:\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot input stream\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    569\u001b[0m                   paCanNotReadFromAnOutputOnlyStream)\n\u001b[1;32m--> 570\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    571\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mexception_on_overflow\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create a Recognizer instance\n",
    "r = sr.Recognizer()\n",
    "\n",
    "# Create an instance of the OpenAI client. The base_url parameter is set to a local server and the api_key is not needed in this case.\n",
    "client = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"not-needed\")\n",
    "\n",
    "# Define the conversation history\n",
    "history = [\n",
    "    {\"role\": \"system\", \"content\": \"I'm going to give you an idea, a prompt for an AI image generator and you are going to enhance that prompt by adding descriptions and keywords\"},\n",
    "]\n",
    "\n",
    "# Use the microphone as the audio source using a with statement because otherwise sr.Microphone() throws an error\n",
    "while True:\n",
    "    with sr.Microphone() as source:\n",
    "        print(\"Say 'Listen AI' to start listening!\")\n",
    "        audio = r.listen(source)\n",
    "\n",
    "        try:\n",
    "            # Use Google's Speech Recognition API to convert the audio to text\n",
    "            text = r.recognize_google(audio)\n",
    "            print(\"You said: \" + text)\n",
    "\n",
    "            if text.lower() == \"listen ai\":\n",
    "                print(\"Listening for your command!\")\n",
    "                audio = r.listen(source)\n",
    "                text = r.recognize_google(audio)\n",
    "                history.append({\"role\": \"user\", \"content\": text})\n",
    "                print(\"Your command: \" + text)\n",
    "\n",
    "\n",
    "                # Call the chat.completions.create method of the client. This method is used to generate a chat completion.\n",
    "                # The model parameter is set to \"local-model\", but it's currently unused.\n",
    "                # The messages parameter is the conversation history. \n",
    "                # The temperature parameter is set to 0.7. This parameter controls the randomness of the model's output. A higher value makes the output more random, while a lower value makes it more deterministic.\n",
    "                completion = client.chat.completions.create(\n",
    "                    model=\"local-model\", \n",
    "                    messages=history,\n",
    "                    temperature=0.7,\n",
    "                )\n",
    "\n",
    "                # Print the message of the first choice from the completion. The choices attribute of a completion contains the different outputs generated by the model.\n",
    "                print(completion.choices[0].message)\n",
    "                history.append({\"role\": \"assistant\", \"content\": completion.choices[0].message['content']})\n",
    "\n",
    "\n",
    "        except sr.UnknownValueError:\n",
    "            print(\"Google Speech Recognition could not understand audio\")\n",
    "        except sr.RequestError as e:\n",
    "            print(\"Could not request results from Google Speech Recognition service; {0}\".format(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOG (VoskAPI:ReadDataFiles():model.cc:213) Decoding params beam=13 max-active=7000 lattice-beam=6\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:216) Silence phones 1:2:3:4:5:11:12:13:14:15\n",
      "LOG (VoskAPI:RemoveOrphanNodes():nnet-nnet.cc:948) Removed 0 orphan nodes.\n",
      "LOG (VoskAPI:RemoveOrphanComponents():nnet-nnet.cc:847) Removing 0 orphan components.\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:248) Loading i-vector extractor from models/vosk-model-en-us-0.22/ivector/final.ie\n",
      "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:183) Computing derived variables for iVector extractor\n",
      "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:204) Done.\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:279) Loading HCLG from models/vosk-model-en-us-0.22/graph/HCLG.fst\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:297) Loading words from models/vosk-model-en-us-0.22/graph/words.txt\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:308) Loading winfo models/vosk-model-en-us-0.22/graph/phones/word_boundary.int\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:315) Loading subtract G.fst model from models/vosk-model-en-us-0.22/rescore/G.fst\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:317) Loading CARPA model from models/vosk-model-en-us-0.22/rescore/G.carpa\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:323) Loading RNNLM model from models/vosk-model-en-us-0.22/rnnlm/final.raw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording and transcribing. Press Ctrl+C to stop.\n",
      "Transcription:  well hello there this is pretty amazing i could not instruct my model to generate the stuff i want to and then it will have context and then i can make it into history and then after that i can just tell him what to change and it will change it\n",
      "Transcription:  and then i have a very nice speech to text with combined other lamps and i image to write that that will be great\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecording and transcribing. Press Ctrl+C to stop.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 31\u001b[0m     \u001b[43msd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/SD/SpeechToImage_Project/.venv/lib/python3.10/site-packages/sounddevice.py:707\u001b[0m, in \u001b[0;36msleep\u001b[0;34m(msec)\u001b[0m\n\u001b[1;32m    700\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msleep\u001b[39m(msec):\n\u001b[1;32m    701\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Put the caller to sleep for at least *msec* milliseconds.\u001b[39;00m\n\u001b[1;32m    702\u001b[0m \n\u001b[1;32m    703\u001b[0m \u001b[38;5;124;03m    The function may sleep longer than requested so don't rely on this\u001b[39;00m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;124;03m    for accurate musical timing.\u001b[39;00m\n\u001b[1;32m    705\u001b[0m \n\u001b[1;32m    706\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 707\u001b[0m     \u001b[43m_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPa_Sleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsec\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Set the sample rate and the number of channels\n",
    "sample_rate = 16000\n",
    "channels = 1\n",
    "\n",
    "# Load the model\n",
    "model = Model(\"models/vosk-model-en-us-0.22\")\n",
    "\n",
    "# Create a recognizer\n",
    "rec = KaldiRecognizer(model, sample_rate)\n",
    "\n",
    "# Define a callback function to process the audio chunks\n",
    "def callback(indata, frames, time, status):\n",
    "    if rec.AcceptWaveform(indata.flatten().tobytes()):\n",
    "        result = rec.Result()\n",
    "        result = json.loads(result)\n",
    "        if result.get('text'):  # Only print when there is text\n",
    "            print(\"Transcription: \", result['text'])\n",
    "\n",
    "# Create a stream to record audio\n",
    "stream = sd.InputStream(callback=callback, channels=channels, samplerate=sample_rate, dtype='int16')\n",
    "\n",
    "# Start recording and processing\n",
    "with stream:\n",
    "    print(\"Recording and transcribing. Press Ctrl+C to stop.\")\n",
    "    while True:\n",
    "        sd.sleep(200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

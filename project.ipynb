{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\LTAM PC\\teinnStuff\\SpeechToImage_Project\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import the OpenAI library. This library provides a Python interface to the OpenAI API.\n",
    "from openai import OpenAI\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "from vosk import Model, KaldiRecognizer\n",
    "import json\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set the sample rate and the number of channels\n",
    "sample_rate = 16000\n",
    "channels = 1\n",
    "\n",
    "# Load the model\n",
    "model = Model(\"models/vosk-model-en-us-0.22\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording and transcribing. Press Ctrl+C to stop.\n",
      "A lush forest at sunrise, with a small cabin nestled in the trees. A couple emerging from the woods, holding hands. Keywords: forest, cabin, sunrise, couple, hand-holding.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 54\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecording and transcribing. Press Ctrl+C to stop.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m---> 54\u001b[0m     \u001b[43msd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\LTAM PC\\teinnStuff\\SpeechToImage_Project\\.venv\\Lib\\site-packages\\sounddevice.py:707\u001b[0m, in \u001b[0;36msleep\u001b[1;34m(msec)\u001b[0m\n\u001b[0;32m    700\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msleep\u001b[39m(msec):\n\u001b[0;32m    701\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Put the caller to sleep for at least *msec* milliseconds.\u001b[39;00m\n\u001b[0;32m    702\u001b[0m \n\u001b[0;32m    703\u001b[0m \u001b[38;5;124;03m    The function may sleep longer than requested so don't rely on this\u001b[39;00m\n\u001b[0;32m    704\u001b[0m \u001b[38;5;124;03m    for accurate musical timing.\u001b[39;00m\n\u001b[0;32m    705\u001b[0m \n\u001b[0;32m    706\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 707\u001b[0m     \u001b[43m_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPa_Sleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsec\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Create an instance of the OpenAI client. The base_url parameter is set to a local server and the api_key is not needed in this case.\n",
    "client = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"not-needed\")\n",
    "\n",
    "# Create a recognizer\n",
    "rec = KaldiRecognizer(model, sample_rate)\n",
    "\n",
    "# Define a global variable to keep track of the recording state\n",
    "global is_recording\n",
    "is_recording = True\n",
    "\n",
    "# Define the conversation history\n",
    "history = [\n",
    "    {\"role\": \"system\", \"content\": \"I'm going to give you an image idea and you are going to return me a creative and enhanced description with keywords of the image you imagine. The description should not be longer than 30 words and be specific and discriptive. Dont be afraid to be creative\"},\n",
    "]\n",
    "\n",
    "# Define a callback function to process the audio chunks\n",
    "def callback(indata, frames, time, status):\n",
    "    global is_recording\n",
    "    if rec.AcceptWaveform(indata.flatten().tobytes()):\n",
    "        result = rec.Result()\n",
    "        result = json.loads(result)\n",
    "        if result.get('text'):  # Only print when there is text\n",
    "            if \"transcription pause\" in result['text']:\n",
    "                is_recording = False\n",
    "            elif \"transcription start\" in result['text']:\n",
    "                is_recording = True\n",
    "                return  # Skip printing the \"transcription start\" command\n",
    "            \n",
    "            if is_recording:\n",
    "                process_user_input(result['text'])\n",
    "\n",
    "# Create a stream to record audio\n",
    "stream = sd.InputStream(callback=callback, channels=channels, samplerate=sample_rate, dtype='int16')\n",
    "\n",
    "def process_user_input(user_input):\n",
    "    # Append user's request to history\n",
    "    history.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "    # Generate response\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"local-model\", # this field is currently unused\n",
    "        messages=history,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    # Print and append assistant's response to history\n",
    "    print(completion.choices[0].message.content)\n",
    "    history.append({\"role\": \"assistant\", \"content\": completion.choices[0].message.content})\n",
    "\n",
    "\n",
    "# Start recording and processing\n",
    "with stream:\n",
    "    print(\"Recording and transcribing. Press Ctrl+C to stop.\")\n",
    "    while True:\n",
    "        sd.sleep(200)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [28/Mar/2024 13:59:16] \"GET / HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, render_template, request, redirect, url_for\n",
    "\n",
    "app = Flask(__name__)\n",
    "@app.route('/')\n",
    "def home():\n",
    "    return render_template('index.html')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
